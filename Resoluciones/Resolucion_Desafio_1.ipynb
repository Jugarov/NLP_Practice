{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "## Consignas del desafío 1\n",
        "\n",
        "### 1. Vectorizar documentos\n",
        "\n",
        "Tomar 5 documentos al azar y medir similaridad con el resto de los documentos. Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "---\n",
        "\n",
        "Para resolver esta consigna lo importante es, en primer lugar, obtener un dataset con documentos variados y vectorizar los documentos usando la técnica TF-IDF. Al igual que lo mostrado en clase, se recurre al dataset de documentos [20 News Groups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) que ya está disponible en las librerías de *Scikit Learn* y contiene 18000 documentos distintos categorizados en 20 tópicos ya separados en conjuntos de entrenamiento y validación. La carga de los datos es como se muestra a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La vectorización mediante TF-IDF (*Term Frequency - Inverse Document Frequency*) consiste en la generación de una lista de **vocabulario** compuesta de todas las palabras presentes en todos los documentos sobre la que se cuenta la frecuencia de aparición de cada palabra en cada documento, de modo que se puede relacionar cada documento con un vector (TF). A continuación se evalúa qué tan informativa o única es cada palabra del vocabulario de modo que las palabras que aparezcan menos en el corpus completo presenten un mayor índice (IDF). El índice IDF de una palabra $n$ es tradicionalmente calculado como:\n",
        "\n",
        "$$\n",
        "IDF(n) = log(\\frac{N}{DF(n)})\n",
        "$$\n",
        "\n",
        "Donde $N$ es la cantidad total de documentos en el corpus y $DF(n)$ la cantidad de documentos que contienen al menos una aparición del término evaluado.\n",
        "\n",
        "Finalmente, para completar la caracterización del vectorizado de cada documento se multiplica el vector TF de cada documento por el valor del índice IDF de cada palabra en el vector.\n",
        "\n",
        "Todo este proceso puede ser realizado utilizando vectorizadores ya provistos por *Scikit Learn* como se muestra a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La matriz obtenida es también llamada **\"Matriz documento-término\"**.\n",
        "\n",
        "Para elegir 5 documentos al azar simplementese utiliza el método `random.sample` para obtener 5 índices aleatorios sobre los que extraer los documentos.\n",
        "\n",
        "Para observar la similitud entre cada documento elegido y el resto del corpus se puede utilizar la similitud del coseno entre el vector elegido y los demás para encontrar los vectores más cercanos entre sí. La función `cosine_similarity` permite hacer el cálculo de manera óptima; y si luego se ordenan los valores de la lista resultante se pueden obtener los 5 documentos más similares al evaluado para ver si la etiqueta asignada es la correcta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== Documento N°1 ======\n",
            "\n",
            "Índice del documento aleatorio: 9605\n",
            "Texto (acotado): Um, the header said *career.* Hodapp managed about 3000 PA in his nine years in the majors. As for his \"consistently over .300,\" make that \"three years in a row, preceded by a part-time year, plus his last year, with Boston.\" Hodapp only qualified for the batting title five times.\n",
            "Grupo: rec.sport.baseball\n",
            "\n",
            "--- Documentos similares ---\n",
            "\n",
            "Índice: 1651\n",
            "Texto (acotado): And some comments, with some players deleted. Yep, that Kevin Mitchell. I never would have expected him in the #1 spot. It's no accident that the first two names are 1988 only. As with first and second base, 1988 was the year of the glove. Average DA was 20 points\n",
            "Grupo: rec.sport.baseball\n",
            "Similitud: 0.25\n",
            "\n",
            "Índice: 4221\n",
            "Texto (acotado): I am trying to think how to respond to this without involving personal feeling or perceptions and I can not without having stats to back up my points. However, I think you approached this the wrong way. I believe all of the people mentioned here deserve the hall of fame\n",
            "Grupo: rec.sport.baseball\n",
            "Similitud: 0.22\n",
            "\n",
            "Índice: 7780\n",
            "Texto (acotado): Well, Maddux looked excellent as the Braves shutout the Cubs 1 - 0. Justice drove in the only run with an RBI single in the first. Get ready for him to have a monster year. He is now hitting the ball to the opposite field with a lot of power\n",
            "Grupo: rec.sport.baseball\n",
            "Similitud: 0.21\n",
            "\n",
            "Índice: 514\n",
            "Texto (acotado): [This is a co-authored report from two of us who were there.] Gun Owners Action League, our state rifle association, started the day with a rally in the secluded courtyard behind the statehouse at 9:30. It was looking sparse (about 40 people) until the speaker began, whereupon about 120 more\n",
            "Grupo: talk.politics.guns\n",
            "Similitud: 0.21\n",
            "\n",
            "Índice: 8717\n",
            "Texto (acotado): Depends on what you mean by \"for a living\". 1974 was the year he led the league in pinch-hit at bats with 50, but he'd been getting a lot of PH ABs earlier than that, and was never a full-time player. 20-35 PH ABs early in his career, 15-20 a\n",
            "Grupo: rec.sport.baseball\n",
            "Similitud: 0.20\n",
            "\n",
            "====== Documento N°2 ======\n",
            "\n",
            "Índice del documento aleatorio: 10103\n",
            "Texto (acotado): I have noticed in FrameMaker 3.1X on both the SGI and SUN platforms that certain dialogs, such as \"Column Layout...\" for example, respond to keyboard traversal even though the pointer is NOT in the dialog window and even though the window manager keyboard focus policy is POINTER. How is this\n",
            "Grupo: comp.windows.x\n",
            "\n",
            "--- Documentos similares ---\n",
            "\n",
            "Índice: 1358\n",
            "Texto (acotado): I *think* this is correct behavior. Remember the default colormapFocusPolicy is keyboard (meaning the cmap focus follows the keyboard focus). Since the dialog is modal, mwm won't allow keyboard focus onto your main shell, and so it won't allow cmap focus either. Since it sounds as though you have keyboardFocusPolicy:pointer,\n",
            "Grupo: comp.windows.x\n",
            "Similitud: 0.45\n",
            "\n",
            "Índice: 5653\n",
            "Texto (acotado): I have two Motif Widgets. I would like to control one of them via the keyboard and the other with the mouse. I set the keyboard focus on the first widget, but as soon as I click the mouse on the second one, I lose the keyboard focus on the\n",
            "Grupo: comp.windows.x\n",
            "Similitud: 0.43\n",
            "\n",
            "Índice: 5447\n",
            "Texto (acotado): Archive-name: typing-injury-faq/keyboards Version: $Revision: 5.11 $ $Date: 1993/04/13 01:20:43 $ ------------------------------------------------------------------------------- Answers To Frequently Asked Questions about Keyboard Alternatives ------------------------------------------------------------------------------- The Alternative Keyboard FAQ Copyright 1992,1993 By Dan Wallach <dwallach@cs.berkeley.edu> The opinions in here are my own, unless otherwise mentioned, and do not represent the opinions of any organization or\n",
            "Grupo: sci.med\n",
            "Similitud: 0.34\n",
            "\n",
            "Índice: 10063\n",
            "Texto (acotado): Hi I am trying to implement a pointer feature in Xlib I have multiple windows and all can take input and show output simultaneously on all other displays I want to implement a pointer feature I would like to get the pointer to come up on all windows once I\n",
            "Grupo: comp.windows.x\n",
            "Similitud: 0.30\n",
            "\n",
            "Índice: 1433\n",
            "Texto (acotado): Hi I am trying to implement a pointer feature in Xlib I have multiple windows and all can take input and show output simultaneously on all other displays I want to implement a pointer feature I would like to get the pointer to come up on all windows once I\n",
            "Grupo: comp.windows.x\n",
            "Similitud: 0.29\n",
            "\n",
            "====== Documento N°3 ======\n",
            "\n",
            "Índice del documento aleatorio: 2801\n",
            "Texto (acotado): Don't swallow propaganda as truth Sir. British promised to Venizelos (greek PM) that mainly greek populated areas of the Ottomans will be given to Greece, _if_ he will agree to drag Greece in the side of the British during the WWI (because the greek King was proGerman). The British succeeded\n",
            "Grupo: talk.politics.mideast\n",
            "\n",
            "--- Documentos similares ---\n",
            "\n",
            "Índice: 503\n",
            "Texto (acotado): Dr. Goebels thought that a lie repeated enough times could finally be believed. I have been observing that 'Poly' has been practicing Goebels' rule quite loyally. 'Poly's audience is mostly made of Greeks who are not allowed to listen to Turkish news. However, in today's informed world Greek propagandists can\n",
            "Grupo: talk.politics.mideast\n",
            "Similitud: 0.38\n",
            "\n",
            "Índice: 6880\n",
            "Texto (acotado): SECRET PURPOSE OF FALKLANDS WAR; [with IN-VISIBILITY Technology] Dr. Beter AUDIO LETTER #74 of 80 Digitized by Jon Volkoff, mail address eidetics@cerf.net \"AUDIO LETTER(R)\" is a registered trademark of Audio Books, Inc., a Texas corporation, which originally produced this tape recording. Reproduced under open license granted by Audio Books, Inc.\n",
            "Grupo: talk.politics.misc\n",
            "Similitud: 0.38\n",
            "\n",
            "Índice: 70\n",
            "Texto (acotado): : Pardon me? Here is to an amherst-clown: : : \"Your three chiefs, Dro, Hamazasp and Kulkhandanian are the ringleaders : of the bands which have destroyed Tartar villages and have staged : massacres in Zangezour, Surmali, Etchmiadzin, and Zangibasar. This is : intolerable. Were you expecting a different response?\n",
            "Grupo: talk.politics.mideast\n",
            "Similitud: 0.36\n",
            "\n",
            "Índice: 10328\n",
            "Texto (acotado): Are you related to 'Arromdian' of ASALA/SDPA/ARF Terrorism and Revisionism Triangle? Ditto. HELSINKI WATCH: \"PROBLEMS OF TURKS IN WESTERN THRACE CONTINUE\" Ankara (A.A) In a 15-page report of the \"Helsinki Watch\" it is stated that the Turkish minority in Western Thrace is still faced with problems and stipulated that the\n",
            "Grupo: talk.politics.mideast\n",
            "Similitud: 0.35\n",
            "\n",
            "Índice: 5423\n",
            "Texto (acotado): Anytime. Suffering from a severe case of myopia? No Muslim left alive - not a single one. Leading the first Armenian units who crossed the Ottoman border in the company of the Russian invaders was the former Ottoman Parliamentary representative for Erzurum, Karekin Pastirmaciyan, who assumed the revolutionary name Armen\n",
            "Grupo: talk.politics.mideast\n",
            "Similitud: 0.35\n",
            "\n",
            "====== Documento N°4 ======\n",
            "\n",
            "Índice del documento aleatorio: 8139\n",
            "Texto (acotado): There was an article in one of the Toronto papers about this a few months ago...probably the Globe and Mail... ...any ethical \"journalist\", even a sports journalist should not accept free meals from a team in any case, which was the one of the points the article was making. Admittdly,\n",
            "Grupo: rec.sport.hockey\n",
            "\n",
            "--- Documentos similares ---\n",
            "\n",
            "Índice: 2599\n",
            "Texto (acotado): I seem to recall that there was an article in Radio Electronics about this subject. In fact I have a copy of the article in front of me, but I can't find anywhere in the article a refrence as to what month it was in. The system they describe uses\n",
            "Grupo: sci.med\n",
            "Similitud: 0.20\n",
            "\n",
            "Índice: 3568\n",
            "Texto (acotado): In <lsjc8cINNmc1@saltillo.cs.utexas.edu> turpin@cs.utexas.edu (Russell Turpin) I regard love as no more or less \"benign\" than any other Christian does. You are merely expressing \"approval\" of the consequences I find therein. Which says more about our politics and cultural trappings than about my (or any) religion. \"Love\" is a highly ambiguous\n",
            "Grupo: talk.religion.misc\n",
            "Similitud: 0.19\n",
            "\n",
            "Índice: 5148\n",
            "Texto (acotado): There was an article in Business week not more the 4 weeks ago on this very subject. IN fact the Volvo 850 was one of the cars they laid out an example for.\n",
            "Grupo: rec.autos\n",
            "Similitud: 0.18\n",
            "\n",
            "Índice: 6928\n",
            "Texto (acotado): The FAN is an okay Sports Radio station, but doesn't come close to the ULTIMATE in Sports Radio, 610 WIP in Philadelphia. The signal might not be as powerful, but then again only stations in New York feel \"obligated\" to pollute everyone else's airwaves with a bunch of hoodlum Mets\n",
            "Grupo: rec.sport.baseball\n",
            "Similitud: 0.18\n",
            "\n",
            "Índice: 9629\n",
            "Texto (acotado): There was a news article a little while ago reporting a type of car (was it a Volvo?) was found to stall if you used a certain brand/model of cellular phone in it. I seem to remember the car was recalled to fix the problem.\n",
            "Grupo: sci.electronics\n",
            "Similitud: 0.18\n",
            "\n",
            "====== Documento N°5 ======\n",
            "\n",
            "Índice del documento aleatorio: 3137\n",
            "Texto (acotado): ------------- cut here ----------------- University of Arizona Tucson, Arizona Suggested Reading Tan SL, Royston P, Campbell S, Jacobs HS, Betts J, Mason B, Edwards RG (1992). Cumulative conception and Livebirth rates after in-vitro fertilization. Lancet 339:1390-1394. For further information, call: Physicians' Resource Line 1-800-328-5868 in Tucson: 694-5868 HICNet Medical Newsletter\n",
            "Grupo: sci.med\n",
            "\n",
            "--- Documentos similares ---\n",
            "\n",
            "Índice: 7243\n",
            "Texto (acotado): A good source of information on Burzynski's method is in *The Cancer Industry* by pulitzer-prize nominee Ralph Moss. Also, a non-profit organization called \"People Against Cancer,\" which was formed for the purpose of allowing cancer patients to access information regarding cancer therapies not endorsed by the cancer industry, but which\n",
            "Grupo: sci.med\n",
            "Similitud: 0.38\n",
            "\n",
            "Índice: 6814\n",
            "Texto (acotado): ------------- cut here ----------------- limits of AZT's efficacy and now suggest using the drug either sequentially with other drugs or in a kind of AIDS treatment \"cocktail\" combining a number of drugs to fight the virus all at once. \"Treating people with AZT alone doesn't happen in the real world\n",
            "Grupo: sci.med\n",
            "Similitud: 0.38\n",
            "\n",
            "Índice: 11156\n",
            "Texto (acotado): ------------- cut here ----------------- HICNet Medical Newsletter Page 13 Volume 6, Number 10 April 20, 1993 Gonorrhea -- Colorado, 1985-1992 ================================ SOURCE: MMWR 42(14) DATE: Apr 16, 1993 The number of reported cases of gonorrhea in Colorado increased 19.9% from 1991 to 1992 after declining steadily during the 1980s. In\n",
            "Grupo: sci.med\n",
            "Similitud: 0.36\n",
            "\n",
            "Índice: 2345\n",
            "Texto (acotado): The biggest reason why the cost of medical care is so EXTREMELY high and increasing is that NATURAL methods of treatment and even diagnosis are still being SYSTEMATICALLY IGNORED and SUPPRESSED by the MONEY-GRUBBING and POWER- MONGERING \"medical\" establishment. Some examples of very low cost NATURAL ANTI-cancer Remedies are listed\n",
            "Grupo: sci.med\n",
            "Similitud: 0.34\n",
            "\n",
            "Índice: 750\n",
            "Texto (acotado): ------------- cut here ----------------- Volume 6, Number 10 April 20, 1993 +------------------------------------------------+ ! ! ! Health Info-Com Network ! ! Medical Newsletter ! +------------------------------------------------+ Editor: David Dodell, D.M.D. 10250 North 92nd Street, Suite 210, Scottsdale, Arizona 85258-4599 USA Telephone +1 (602) 860-1121 FAX +1 (602) 451-1165 Compilation Copyright 1993 by\n",
            "Grupo: sci.med\n",
            "Similitud: 0.30\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Select 5 random document indices\n",
        "random_indices = random.sample(range(X_train.shape[0]), 5)\n",
        "\n",
        "i = 0\n",
        "for idx in random_indices:\n",
        "    i += 1\n",
        "    print(f\"\\n====== Documento N°{i} ======\")\n",
        "\n",
        "    # Calculate cosine similarity between the selected document and all others\n",
        "    similarities = cosine_similarity(X_train[idx], X_train).flatten()\n",
        "\n",
        "    # Get indices of the 5 most similar documents (excluding itself)\n",
        "    most_similar = similarities.argsort()[::-1][1:6]\n",
        "    \n",
        "    print(f\"\\nÍndice del documento aleatorio: {idx}\")\n",
        "    print(\"Texto (acotado):\", ' '.join(newsgroups_train.data[idx].split()[:50]))\n",
        "    print(\"Grupo:\", newsgroups_train.target_names[newsgroups_train.target[idx]])\n",
        "    \n",
        "    print(\"\\n--- Documentos similares ---\")\n",
        "    for sim_idx in most_similar:\n",
        "        print(f\"\\nÍndice: {sim_idx}\")\n",
        "        print(\"Texto (acotado):\", ' '.join(newsgroups_train.data[sim_idx].split()[:50]))\n",
        "        print(\"Grupo:\", newsgroups_train.target_names[newsgroups_train.target[sim_idx]])\n",
        "        print(f\"Similitud: {similarities[sim_idx]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En líneas generales, se observa que los valores de similitud entre documentos no son muy elevados, pero el grupo en el que se clasifican el elemento aleatorio y los que se han calculado ser más similares suelen ser el mismo. Leyendo las frases extraídas de cada documento evaluado para cada experimento uno puede observar que tienden a hablar del mismo tópico, confirmando la clasificación en los grupos etiquetados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "### **2**. Construir un modelo de clasificación por prototipos (tipo zero-shot)\n",
        "\n",
        "Clasificar los documentos de un conjunto de test comparando cada uno con todos los de entrenamiento y asignar la clase al label del documento del conjunto de entrenamiento con mayor similaridad.\n",
        "\n",
        "---\n",
        "\n",
        "Para cumplir esta consigna simplemente se compara la similitud del coseno de cada elemento del conjunto de validación con el corpus de entrenamiento y se otorga al elemento de evaluación el mismo grupo que el del elemento más cercano encontrado (un análogo a un algoritmo KNN con $K=1$ que usa la similitud del coseno como distancia entre elementos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Certeza en la clasificación del conjunto de validación: 0.5089\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "predicted_labels = []\n",
        "for x_test_vec in X_test:\n",
        "    sims = cosine_similarity(x_test_vec, X_train).flatten()\n",
        "\n",
        "    # Get index of the most similar document\n",
        "    most_similar_idx = np.argmax(sims)\n",
        "    predicted_labels.append(newsgroups_train.target[most_similar_idx])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f\"\\nCerteza en la clasificación del conjunto de validación: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este método ruidimentario es muy sensible a ruido en las clasificaciones encontradas, pero es simple y permite constituir un baseline con el que contrastar otros modelos en el práctico. El valor que se apunta a superar es el 50.89% de certeza global (u otra métrica mejor para la clasificación multigrupo como el F1-score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3**. Entrenar modelos de clasificación Naïve Bayes \n",
        "\n",
        "Maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB.\n",
        "\n",
        "---\n",
        "\n",
        "Nuevamente, *Scikit Learn* provee modelos de clasificadores que se pueden entrenar para evaluar el desempeño. Se generan varias versiones de estos modelos, jugando con sus parámetros y comparando los resultados obtenidos, medidos de acuerdo a la puntuación F1.\n",
        "\n",
        "Además, se hacen pruebas con modificando los siguientes parámetros del vectorizador:\n",
        "\n",
        "* `ngram_range`: Cantidad de palabras evaluadas a la vez\n",
        "\n",
        "* `min_df` y `min_df`: Controlan la frecuencia mínima y máxima, respectivamente, con la que debe aparecer una palabra en el corpus para ser considerada por el modelo\n",
        "\n",
        "* `stop_words`: Eliminan términos contenidos en un diccionario con palabras de uso común que no aportan para diferenciar un documento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Vectorizador original =====\n",
            "\n",
            "--- Modelo MultinomialNB ---\n",
            "Mejor alpha: {'alpha': 0.1}\n",
            "Mejor F1-macro (train): 0.7188497427747561\n",
            "F1-macro (test): 0.656\n",
            "\n",
            "--- Modelo ComplementNB ---\n",
            "Mejor alpha: {'alpha': 0.1}\n",
            "Mejor F1-macro (train): 0.7656097292010164\n",
            "F1-macro (test): 0.695\n",
            "===== Vectorizador modificado =====\n",
            "\n",
            "--- Modelo MultinomialNB ---\n",
            "Mejor alpha: {'alpha': 0.1}\n",
            "Mejor F1-macro (train): 0.7521240433003771\n",
            "F1-macro (test): 0.680\n",
            "\n",
            "--- Modelo ComplementNB ---\n",
            "Mejor alpha: {'alpha': 0.5}\n",
            "Mejor F1-macro (train): 0.7665934794124383\n",
            "F1-macro (test): 0.703\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Searching grid for the best alpha parameter for MultinomialNB & ComplementNB\n",
        "param_grid = {\n",
        "    \"alpha\": [0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# State all models to train and evaluate\n",
        "models = {\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"ComplementNB\": ComplementNB()\n",
        "}\n",
        "\n",
        "# Function to train and evaluate a model using GridSearchCV\n",
        "def train_and_evaluate(model, model_name, X_train, y_train, X_test, y_test):\n",
        "    print(f\"\\n--- Modelo {model_name} ---\")\n",
        "\n",
        "    # Use GridSearchCV to find the best alpha\n",
        "    grid = GridSearchCV(model, param_grid, scoring=\"f1_macro\", cv=5)\n",
        "    grid.fit(X_train, y_train)\n",
        "    print(\"Mejor alpha:\", grid.best_params_)\n",
        "    print(\"Mejor F1-macro (train):\", grid.best_score_)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    model = grid.best_estimator_\n",
        "    y_pred = model.predict(X_test)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "    print(f\"F1-macro (test): {f1:.3f}\")\n",
        "\n",
        "# Training and evaluating each model\n",
        "print(\"===== Vectorizador original =====\")\n",
        "for name, model in models.items():\n",
        "    train_and_evaluate(model, name, X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Modifying the vectorizer with new parameters\n",
        "modified_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_df=0.9, min_df=3, ngram_range=(1,2))\n",
        "X_train_modified = modified_vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test_modified = modified_vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "# Use new encodings to train and evaluate each model again\n",
        "print(\"===== Vectorizador modificado =====\")\n",
        "for name, model in models.items():\n",
        "    train_and_evaluate(model, name, X_train_modified, y_train, X_test_modified, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados obtenidos se sintetizan en la siguiente tabla:\n",
        "\n",
        "| Modelo                                  | Mejor α | F1-macro (train) | F1-macro (test) |\n",
        "|-----------------------------------------|---------|------------------|-----------------|\n",
        "| MultinomialNB (Vectorizador Original)   | 0.1     | 0.719            | 0.656           |\n",
        "| ComplementNB (Vectorizador Original)    | 0.1     | 0.766            | 0.695           |\n",
        "| MultinomialNB (Vectorizador Modificado) | 0.1     | 0.752            | 0.680           |\n",
        "| ComplementNB (Vectorizador Modificado)  | 0.5     | 0.767            | 0.703           |\n",
        "\n",
        "En cualquiera de estas versiones se obtienen mejores resultados que el clasificador zero-shot implementado inicialmente por un margen de mejora de entre el 30 y el 40%. De entre los modelos evaluados se observa mejores resultados en aquellos que usan el vectorizador modificado tanto en el conjunto de entrenamiento como en el de evaluación, destacando la obtención de un modelo con un **70%** de certeza en el conjunto de evaluación cuando las mejoras en el vectorizador se han aplicado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **4**. Transponer la matriz documento-término\n",
        "\n",
        "De esa manera se obtiene una matriz término-documento que puede ser interpretada como una colección de vectorización de palabras. Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**.\n",
        "\n",
        "---\n",
        "\n",
        "Para poder cumplir esto simplemente se realiza el mismo proceso que se hizo para detectar la similitud entre documentos (cada documento es representado por una fila) usando la similtud del coseno pero comparando palabras esta vez (cada palabra es representada por una columna). Esto se logra simplemente transponiendo la matriz documento-término que se obtuvo con anterioridad.\n",
        "\n",
        "Se usará para esto la vectorización modificada que se generó en el ejercicio anterior, ya que contiene un contexto reducido y mejor tratado de las palabras en el vocabulario. Además, como se preparó la vectorización para consumir las palabras de a pares, las palabras evaluadas podrán ser devueltas también de a pares, entendiendo mejor el contexto en el que se suelen usar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Palabra base: computer\n",
            "   - computer science (sim=0.225)\n",
            "   - computer graphics (sim=0.220)\n",
            "   - turn computer (sim=0.207)\n",
            "   - computer equipment (sim=0.176)\n",
            "   - new computer (sim=0.169)\n",
            "\n",
            "Palabra base: devil\n",
            "   - honig (sim=0.336)\n",
            "   - copenhagen denmark (sim=0.335)\n",
            "   - copenhagen (sim=0.318)\n",
            "   - diku dk (sim=0.317)\n",
            "   - diku (sim=0.317)\n",
            "\n",
            "Palabra base: guns\n",
            "   - gun (sim=0.388)\n",
            "   - politics guns (sim=0.329)\n",
            "   - talk politics (sim=0.272)\n",
            "   - machine guns (sim=0.261)\n",
            "   - make guns (sim=0.245)\n",
            "\n",
            "Palabra base: conflict\n",
            "   - variety sources (sim=0.324)\n",
            "   - hardware conflict (sim=0.294)\n",
            "   - israeli palestinian (sim=0.277)\n",
            "   - calamity (sim=0.276)\n",
            "   - religious differences (sim=0.275)\n",
            "\n",
            "Palabra base: bat\n",
            "   - autoexec bat (sim=0.436)\n",
            "   - autoexec (sim=0.431)\n",
            "   - bat file (sim=0.305)\n",
            "   - config sys (sim=0.294)\n",
            "   - sys autoexec (sim=0.262)\n"
          ]
        }
      ],
      "source": [
        "# Selection of 5 specific words to analyze their 5 most context-similar words\n",
        "word_scope = [\"computer\", \"devil\", \"guns\", \"conflict\", \"bat\"]\n",
        "\n",
        "# Transpose the modified TF-IDF matrix to get term-document matrix & calculate word-similarities\n",
        "similarities = cosine_similarity(X_train_modified.T)\n",
        "\n",
        "# List words on the vocabulary\n",
        "feature_names = modified_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Find and print the 5 most similar words for each word in word_scope\n",
        "for word in word_scope:\n",
        "    if word in feature_names:\n",
        "        idx = list(feature_names).index(word)\n",
        "        sims = similarities[idx]\n",
        "        top5_idx = np.argsort(sims)[::-1][1:6]  # excluye la misma palabra\n",
        "        print(f\"\\nPalabra base: {word}\")\n",
        "        for i in top5_idx:\n",
        "            print(f\"   - {feature_names[i]} (sim={sims[i]:.3f})\")\n",
        "    \n",
        "    else:\n",
        "        print(f\"La palabra '{word}' no está en el vocabulario.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se observa que los términos encontrados con mayor similitud incluyen el uso de expresiones, lugares, disciplinas o acepciones de las palabras que no se consideraron originalmente. Complementariamente se incorpora la vectorización original para ver cómo cambia el contexto de las palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Palabra base: computer\n",
            "   - decwriter (sim=0.156)\n",
            "   - harkens (sim=0.152)\n",
            "   - deluged (sim=0.152)\n",
            "   - shopper (sim=0.144)\n",
            "   - the (sim=0.136)\n",
            "\n",
            "Palabra base: devil\n",
            "   - cec2 (sim=0.389)\n",
            "   - mvs1 (sim=0.389)\n",
            "   - honig (sim=0.355)\n",
            "   - copenhagen (sim=0.339)\n",
            "   - jesper (sim=0.335)\n",
            "\n",
            "Palabra base: guns\n",
            "   - gun (sim=0.358)\n",
            "   - preyed (sim=0.287)\n",
            "   - mcgrath (sim=0.253)\n",
            "   - saloons (sim=0.253)\n",
            "   - iftccu (sim=0.243)\n",
            "\n",
            "Palabra base: conflict\n",
            "   - isreali (sim=0.314)\n",
            "   - poisons (sim=0.314)\n",
            "   - colonized (sim=0.314)\n",
            "   - colonialist (sim=0.314)\n",
            "   - collectivities (sim=0.314)\n",
            "\n",
            "Palabra base: bat\n",
            "   - autoexec (sim=0.526)\n",
            "   - varibles (sim=0.326)\n",
            "   - config (sim=0.278)\n",
            "   - sys (sim=0.261)\n",
            "   - theese (sim=0.173)\n"
          ]
        }
      ],
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "word_to_idx = {word: i for i, word in enumerate(feature_names)}\n",
        "\n",
        "# Undersample word_scope to only those present in the vocabulary to reduce the matrix size\n",
        "valid_words = [w for w in word_scope if w in word_to_idx]\n",
        "indices = [word_to_idx[w] for w in valid_words]\n",
        "\n",
        "# Take column vectors\n",
        "submatrix = X_train.T[indices]\n",
        "\n",
        "# Calculate similarities\n",
        "sims = cosine_similarity(submatrix, X_train.T)\n",
        "\n",
        "for i, word in enumerate(valid_words):\n",
        "    row = sims[i]\n",
        "    top5_idx = np.argsort(row)[::-1][1:6]\n",
        "    print(f\"\\nPalabra base: {word}\")\n",
        "    for j in top5_idx:\n",
        "        print(f\"   - {feature_names[j]} (sim={row[j]:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se observa que, si bien hay algunos términos nuevos que aparecen, el contexto general para cada término es similar al anterior."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
