{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8744387",
   "metadata": {},
   "source": [
    "## Consignas del Desafío 2\n",
    "\n",
    "### 1. Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
    "\n",
    "Para ello se descarga el dataset `britney-spears.txt` provisto por el enlace dejado por la cátedra y se recopila un dataset propio `taylor_swift_lirics.txt` usando la API pública de Genius. A continuación se leen los archivos utilizando el caracter `/n` para separar cada oración de las canciones contenidas en el documento y conformar así un dataframe que contenga esta información. Cada oración es un *documento* en lo que respecta a la aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbe4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos (Britney): 3848\n",
      "Cantidad de documentos (Taylor): 3899\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Read datasets using newline as separator\n",
    "britney_df = pd.read_csv('./songs_dataset/britney-spears.txt', sep='/n', header=None, engine='python')\n",
    "britney_df.head()\n",
    "\n",
    "taylor_df = pd.read_csv('./songs_dataset/taylor_swift_lyrics.txt', sep='/n', header=None, engine='python')\n",
    "taylor_df.head()\n",
    "\n",
    "# Count the number of documents in each dataset\n",
    "print(\"Cantidad de documentos (Britney):\", britney_df.shape[0])\n",
    "print(\"Cantidad de documentos (Taylor):\", taylor_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21774cf",
   "metadata": {},
   "source": [
    "A continuación se arma el vocabulario para cada artista, filtrando palabras de uso común que no aportan información relevante a través del uso de la librería `nltk`. Cada palabra es llamada *token* en este contexto; y la detección de los mismos dentro de un documento es realizada por una librería propia de `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32b3e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JuanI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['say', 'get', 'ready', 'revolution'], ['think', 'time', 'find', 'sorta', 'solution'], [\"somebody's\", 'caught', 'endless', 'pollution'], ['need', 'wake', 'stop', 'living', 'illusions', 'know', 'need', 'hear'], ['somebody', 'feel'], ['wish', 'feel', 'connected'], ['wish', 'nobodies', 'neglected', 'like', 'rocket', 'baby'], ['like', 'rocket', 'take'], ['fly', 'away', 'ay', 'ay'], ['find', 'space', 'take'], ['fly', 'away', 'ay', 'ay'], ['find', 'place', 'take', 'know', 'say', 'mixing', 'races'], ['end', 'got', 'faces'], ['mama', 'told', 'got', 'love', 'first'], ['disagree', 'get', 'damn', 'earth', 'want', 'feel', 'connected'], ['want', 'neglected'], ['wish', 'find', 'places'], ['wish', 'escalate', 'yeah', 'like', 'rocket', 'baby'], ['like', 'rocket', 'take'], ['fly', 'away', 'ay', 'ay']]\n",
      "[['wanted', 'wanted'], [], ['country', \"lovin'\", 'really', 'something'], ['three', 'months', 'half'], ['everybody', 'said', 'bad', 'news'], ['baby', 'betting'], ['guess', 'owe', 'friends', 'ten', 'dollars', 'six', 'pack'], ['got', 'big', 'country', \"servin'\", 'payback'], ['going', 'listen', 'mother', 'told'], [\"ain't\", 'even', 'worth'], ['ten', 'dollars', 'six', 'pack'], ['said', 'let', 'say', 'crazy'], ['say', \"let's\", 'go', 'fishing', 'lot'], ['midnight', 'moon', \"blazin'\", 'baby'], [\"gazin'\", 'eyes'], ['everything', 'changed', 'took', 'town'], ['caught', 'staring', 'every', 'girl'], ['guess', 'owе', 'friends', 'ten', 'dollars', 'six', 'pack'], ['got', 'big', 'country', \"sеrvin'\", 'payback'], ['going', 'listen', 'mother', 'told']]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Get english stopwords from nltk\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define a function to tokenize text and remove stopwords\n",
    "def tokenize_no_stopwords(text):\n",
    "    tokens = text_to_word_sequence(text)\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "# Tokenize the entire datasets\n",
    "britney_sentence_tokens = [tokenize_no_stopwords(row[0]) for _, row in britney_df.iterrows()]\n",
    "taylor_sentence_tokens = [tokenize_no_stopwords(row[0]) for _, row in taylor_df.iterrows()]\n",
    "    \n",
    "# Show some example tokens\n",
    "print(britney_sentence_tokens[:20])\n",
    "print(taylor_sentence_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f870831",
   "metadata": {},
   "source": [
    "Ahora para el vectorizado de los tokens de cada dataset se usa `gensim` como solicita la consigna. Esta librería provee una técnica llamada *Word2Vec* que implica utilizar un modelo de red neuronal capaz de agrupar tokens por su similitud dentro de un contexto dado. El modelo utilizado en este caso es Skip-gram debido a que permite detectar palabras poco frecuentes y darles más importancia en un contexto dado; en contraposición con un modelo CBOW que funciona más rápido pero no destaca tanto el contexto de una palabra sino las palabras con la que se suele rodear un token dado. Más información sobre el tema se puede encontrar en [el siguiente enlace](https://es.wikipedia.org/wiki/Word2vec#CBOW_y_skip-gram).\n",
    "\n",
    "Del material de clase se recupera la sobrescritura del callback de Any2Vec para escribir el valor de la función de pérdida en cada iteración durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0b18de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de words distintas en el corpus (Britney): 606\n",
      "Cantidad de words distintas en el corpus (Taylor): 752\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# Overwrite callback to print loss after each epoch\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "\n",
    "# Use Word2Vec to create word embeddings. Set the model for the embedding as Skip-gram (sg=1)\n",
    "britney_w2v_model = Word2Vec(min_count=4,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     negative=20,\n",
    "                     workers=4,\n",
    "                     sg=1)\n",
    "\n",
    "taylor_w2v_model = Word2Vec(min_count=4,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     negative=20,\n",
    "                     workers=4,\n",
    "                     sg=1)\n",
    "\n",
    "# Once the model is created, build the vocabulary with the sentence tokens\n",
    "britney_w2v_model.build_vocab(britney_sentence_tokens)\n",
    "taylor_w2v_model.build_vocab(taylor_sentence_tokens)\n",
    "\n",
    "# Words in the vocabulary\n",
    "print(\"Cantidad de words distintas en el corpus (Britney):\", len(britney_w2v_model.wv.index_to_key))\n",
    "print(\"Cantidad de words distintas en el corpus (Taylor):\", len(taylor_w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93e312",
   "metadata": {},
   "source": [
    "Tras crear la arquitectura de los modelos, solo resta entrenarlos para obtener una buena representación vectorizada de las palabras más similares entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a549a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 83378.09375\n",
      "Loss after epoch 1: 41478.640625\n",
      "Loss after epoch 2: 38622.734375\n",
      "Loss after epoch 3: 38829.34375\n",
      "Loss after epoch 4: 37559.796875\n",
      "Loss after epoch 5: 37353.421875\n",
      "Loss after epoch 6: 35987.03125\n",
      "Loss after epoch 7: 35930.09375\n",
      "Loss after epoch 8: 34831.78125\n",
      "Loss after epoch 9: 33310.28125\n",
      "Loss after epoch 10: 31710.4375\n",
      "Loss after epoch 11: 30264.84375\n",
      "Loss after epoch 12: 28505.03125\n",
      "Loss after epoch 13: 26801.53125\n",
      "Loss after epoch 14: 25542.0\n",
      "Loss after epoch 15: 24037.625\n",
      "Loss after epoch 16: 23207.9375\n",
      "Loss after epoch 17: 22434.875\n",
      "Loss after epoch 18: 20974.8125\n",
      "Loss after epoch 19: 20778.75\n",
      "Loss after epoch 20: 19362.75\n",
      "Loss after epoch 21: 19172.125\n",
      "Loss after epoch 22: 18486.9375\n",
      "Loss after epoch 23: 18354.25\n",
      "Loss after epoch 24: 17538.1875\n",
      "Loss after epoch 25: 17394.8125\n",
      "Loss after epoch 26: 16966.625\n",
      "Loss after epoch 27: 16779.3125\n",
      "Loss after epoch 28: 16445.875\n",
      "Loss after epoch 29: 16247.5625\n",
      "Loss after epoch 30: 15806.0625\n",
      "Loss after epoch 31: 15117.6875\n",
      "Loss after epoch 32: 15052.75\n",
      "Loss after epoch 33: 15664.5625\n",
      "Loss after epoch 34: 15347.9375\n",
      "Loss after epoch 35: 15197.0\n",
      "Loss after epoch 36: 14861.625\n",
      "Loss after epoch 37: 14681.0625\n",
      "Loss after epoch 38: 14637.3125\n",
      "Loss after epoch 39: 14427.9375\n",
      "Loss after epoch 40: 14221.5625\n",
      "Loss after epoch 41: 14573.6875\n",
      "Loss after epoch 42: 14688.75\n",
      "Loss after epoch 43: 13735.0625\n",
      "Loss after epoch 44: 13524.25\n",
      "Loss after epoch 45: 13723.0\n",
      "Loss after epoch 46: 13991.25\n",
      "Loss after epoch 47: 13861.625\n",
      "Loss after epoch 48: 13164.125\n",
      "Loss after epoch 49: 14055.5\n",
      "Loss after epoch 50: 13497.25\n",
      "Loss after epoch 51: 13307.75\n",
      "Loss after epoch 52: 13430.375\n",
      "Loss after epoch 53: 13736.625\n",
      "Loss after epoch 54: 13352.25\n",
      "Loss after epoch 55: 13864.875\n",
      "Loss after epoch 56: 12892.375\n",
      "Loss after epoch 57: 13344.5\n",
      "Loss after epoch 58: 13367.375\n",
      "Loss after epoch 59: 13116.875\n",
      "Loss after epoch 60: 13345.875\n",
      "Loss after epoch 61: 13241.0\n",
      "Loss after epoch 62: 13325.5\n",
      "Loss after epoch 63: 12993.75\n",
      "Loss after epoch 64: 13444.625\n",
      "Loss after epoch 65: 13076.875\n",
      "Loss after epoch 66: 13193.375\n",
      "Loss after epoch 67: 13345.75\n",
      "Loss after epoch 68: 13188.75\n",
      "Loss after epoch 69: 13003.875\n",
      "Loss after epoch 70: 13148.75\n",
      "Loss after epoch 71: 12844.0\n",
      "Loss after epoch 72: 12865.5\n",
      "Loss after epoch 73: 12863.75\n",
      "Loss after epoch 74: 12852.375\n",
      "Loss after epoch 75: 12652.875\n",
      "Loss after epoch 76: 12824.375\n",
      "Loss after epoch 77: 13132.125\n",
      "Loss after epoch 78: 12756.75\n",
      "Loss after epoch 79: 12924.625\n",
      "Loss after epoch 80: 13119.25\n",
      "Loss after epoch 81: 12814.625\n",
      "Loss after epoch 82: 13024.875\n",
      "Loss after epoch 83: 13061.375\n",
      "Loss after epoch 84: 12509.5\n",
      "Loss after epoch 85: 13073.375\n",
      "Loss after epoch 86: 12758.125\n",
      "Loss after epoch 87: 12584.125\n",
      "Loss after epoch 88: 12709.5\n",
      "Loss after epoch 89: 12681.125\n",
      "Loss after epoch 90: 12434.625\n",
      "Loss after epoch 91: 12815.625\n",
      "Loss after epoch 92: 12746.375\n",
      "Loss after epoch 93: 12349.5\n",
      "Loss after epoch 94: 12522.125\n",
      "Loss after epoch 95: 12529.5\n",
      "Loss after epoch 96: 12588.875\n",
      "Loss after epoch 97: 12229.625\n",
      "Loss after epoch 98: 12744.125\n",
      "Loss after epoch 99: 12429.375\n",
      "Loss after epoch 0: 110961.0\n",
      "Loss after epoch 1: 46214.859375\n",
      "Loss after epoch 2: 39561.28125\n",
      "Loss after epoch 3: 38935.734375\n",
      "Loss after epoch 4: 38572.0625\n",
      "Loss after epoch 5: 38940.28125\n",
      "Loss after epoch 6: 39075.28125\n",
      "Loss after epoch 7: 39041.75\n",
      "Loss after epoch 8: 38962.6875\n",
      "Loss after epoch 9: 38066.96875\n",
      "Loss after epoch 10: 37170.6875\n",
      "Loss after epoch 11: 36627.40625\n",
      "Loss after epoch 12: 36390.0625\n",
      "Loss after epoch 13: 35596.9375\n",
      "Loss after epoch 14: 34278.1875\n",
      "Loss after epoch 15: 33503.5625\n",
      "Loss after epoch 16: 32037.9375\n",
      "Loss after epoch 17: 30890.3125\n",
      "Loss after epoch 18: 30050.8125\n",
      "Loss after epoch 19: 28488.9375\n",
      "Loss after epoch 20: 27847.9375\n",
      "Loss after epoch 21: 26704.0\n",
      "Loss after epoch 22: 25592.625\n",
      "Loss after epoch 23: 24903.375\n",
      "Loss after epoch 24: 24261.8125\n",
      "Loss after epoch 25: 23484.3125\n",
      "Loss after epoch 26: 22959.25\n",
      "Loss after epoch 27: 22357.1875\n",
      "Loss after epoch 28: 21437.375\n",
      "Loss after epoch 29: 20924.5\n",
      "Loss after epoch 30: 19721.25\n",
      "Loss after epoch 31: 18805.5\n",
      "Loss after epoch 32: 18486.875\n",
      "Loss after epoch 33: 18199.0\n",
      "Loss after epoch 34: 17602.125\n",
      "Loss after epoch 35: 17105.5\n",
      "Loss after epoch 36: 17492.375\n",
      "Loss after epoch 37: 16851.5\n",
      "Loss after epoch 38: 16768.75\n",
      "Loss after epoch 39: 16702.0\n",
      "Loss after epoch 40: 16180.25\n",
      "Loss after epoch 41: 16002.75\n",
      "Loss after epoch 42: 15835.875\n",
      "Loss after epoch 43: 15568.25\n",
      "Loss after epoch 44: 15440.375\n",
      "Loss after epoch 45: 15989.5\n",
      "Loss after epoch 46: 15514.75\n",
      "Loss after epoch 47: 15206.625\n",
      "Loss after epoch 48: 15092.5\n",
      "Loss after epoch 49: 14701.0\n",
      "Loss after epoch 50: 14703.75\n",
      "Loss after epoch 51: 14731.875\n",
      "Loss after epoch 52: 14634.25\n",
      "Loss after epoch 53: 14319.75\n",
      "Loss after epoch 54: 14384.75\n",
      "Loss after epoch 55: 14491.125\n",
      "Loss after epoch 56: 14290.375\n",
      "Loss after epoch 57: 14414.875\n",
      "Loss after epoch 58: 13912.875\n",
      "Loss after epoch 59: 14144.375\n",
      "Loss after epoch 60: 13607.25\n",
      "Loss after epoch 61: 13673.0\n",
      "Loss after epoch 62: 14139.75\n",
      "Loss after epoch 63: 13619.0\n",
      "Loss after epoch 64: 14269.375\n",
      "Loss after epoch 65: 13645.375\n",
      "Loss after epoch 66: 14030.875\n",
      "Loss after epoch 67: 13774.875\n",
      "Loss after epoch 68: 13479.25\n",
      "Loss after epoch 69: 13807.0\n",
      "Loss after epoch 70: 13478.375\n",
      "Loss after epoch 71: 13660.875\n",
      "Loss after epoch 72: 13747.25\n",
      "Loss after epoch 73: 13095.125\n",
      "Loss after epoch 74: 13544.25\n",
      "Loss after epoch 75: 13602.125\n",
      "Loss after epoch 76: 13778.5\n",
      "Loss after epoch 77: 13816.875\n",
      "Loss after epoch 78: 13426.625\n",
      "Loss after epoch 79: 13440.125\n",
      "Loss after epoch 80: 13453.125\n",
      "Loss after epoch 81: 13446.875\n",
      "Loss after epoch 82: 13312.25\n",
      "Loss after epoch 83: 13168.125\n",
      "Loss after epoch 84: 13128.875\n",
      "Loss after epoch 85: 12958.75\n",
      "Loss after epoch 86: 13211.75\n",
      "Loss after epoch 87: 12897.75\n",
      "Loss after epoch 88: 13246.375\n",
      "Loss after epoch 89: 13218.0\n",
      "Loss after epoch 90: 12811.375\n",
      "Loss after epoch 91: 13088.875\n",
      "Loss after epoch 92: 13083.25\n",
      "Loss after epoch 93: 13191.25\n",
      "Loss after epoch 94: 13052.75\n",
      "Loss after epoch 95: 12996.75\n",
      "Loss after epoch 96: 12801.75\n",
      "Loss after epoch 97: 12963.875\n",
      "Loss after epoch 98: 12658.0\n",
      "Loss after epoch 99: 12915.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(888591, 1397500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "britney_w2v_model.train(britney_sentence_tokens,\n",
    "                 total_examples=britney_w2v_model.corpus_count,\n",
    "                 epochs=100,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )\n",
    "\n",
    "taylor_w2v_model.train(taylor_sentence_tokens,\n",
    "                 total_examples=taylor_w2v_model.corpus_count,\n",
    "                 epochs=100,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb61050",
   "metadata": {},
   "source": [
    "### 2. Probar términos de interés y explicar similitudes en el espacio de embeddings\n",
    "\n",
    "Con el modelo ya entrenado, se puede ahora buscar palabras en el vocabulario generado para cada artista y observar cuáles son las palabras más relacionadas con el término buscado (suponiendo que existan). Un ejercicio interesante es mostrar un mismo tema común a ambas artistas y observar cómo cambia el contexto que le da cada una. Se muestra por ejemplo cómo interpreta cada artista el concepto del amor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b42306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('haha', 0.6291143894195557),\n",
       " ('rational', 0.5430219769477844),\n",
       " ('wings', 0.5192895531654358),\n",
       " ('ohhhh', 0.5174964070320129),\n",
       " ('criminal', 0.5089353919029236),\n",
       " ('physical', 0.49342045187950134),\n",
       " ('bombastic', 0.4898693561553955),\n",
       " ('sent', 0.48802709579467773),\n",
       " ('roll', 0.4836137890815735),\n",
       " ('hate', 0.4812115430831909)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "britney_w2v_model.wv.most_similar(positive=[\"love\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23061ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mad', 0.4945010840892792),\n",
       " ('slip', 0.4479531943798065),\n",
       " ('lie', 0.4287341833114624),\n",
       " ('real', 0.4237478971481323),\n",
       " ('used', 0.41310805082321167),\n",
       " (\"why'd\", 0.40498632192611694),\n",
       " ('rough', 0.4026148319244385),\n",
       " ('cry', 0.3930530250072479),\n",
       " ('boarded', 0.39304324984550476),\n",
       " ('fallen', 0.39029765129089355)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taylor_w2v_model.wv.most_similar(positive=[\"love\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdb5e8",
   "metadata": {},
   "source": [
    "Se observa que Britney Spears utiliza el concepto del amor de una forma más pasional, destacando sensaciones y sentimientos extremos y realzados; mientras que Taylor Swift lo lleva más a contextos melancólicos, ligados a conceptos más tristes y situaciones negativas a su causa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd6441",
   "metadata": {},
   "source": [
    "### 3. Graficar espacio de vectores\n",
    "\n",
    "Finalmente, con los vectores generados puede hacerse una representación como una nube de palabras que agrupe los conceptos más usados en conjunto entre sí y separe más las palabras menos similares. Cabe destacar que, como se mencionó en clase, esta es una visualización resultado de una reducción de dimensionalidad usando PCA, por lo que la distancia entre términos tras su transformación puede no resultar tan precisa como medirla en los vectores multidimensionales que se usan originalmente; pero aún así dan una buena idea de la cercanía aparente entre términos.\n",
    "\n",
    "Se limita el dibujo de la nube de palabras a 200 palabras y se abre el gráfico en el navegador para poder tener una interfaz más interactiva (y grande) donde evaluar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb92c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JuanI\\miniconda3\\envs\\NLP\\Lib\\site-packages\\threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# Limiting threads to avoid MKL/OMP conflicts on Windows\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Function to reduce dimensions using PCA + TSNE\n",
    "def reduce_dimensions(model, num_dimensions=3, pca_components=50, max_words=None):\n",
    "    \"\"\"\n",
    "    Reduce dimensionalidad de un modelo Word2Vec a `num_dimensions` para visualización 2D o 3D.\n",
    "    Primero aplica PCA a `pca_components`, luego TSNE.\n",
    "    \n",
    "    Args:\n",
    "        model: Gensim Word2Vec entrenado\n",
    "        num_dimensions: dimensiones finales (2 o 3)\n",
    "        pca_components: dimensiones intermedias para PCA\n",
    "        max_words: número máximo de palabras a procesar (None = todas)\n",
    "        \n",
    "    Returns:\n",
    "        vectors_tsne: np.array de shape (n_words, num_dimensions)\n",
    "        labels: lista de palabras\n",
    "    \"\"\"\n",
    "    # Vectors and its dictionary\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)\n",
    "\n",
    "    # Limit to max_words if specified\n",
    "    if max_words is not None:\n",
    "        vectors = vectors[:max_words]\n",
    "        labels = labels[:max_words]\n",
    "\n",
    "    # Reduce dimensionality with PCA first\n",
    "    pca = IncrementalPCA(n_components=min(pca_components, vectors.shape[1]))\n",
    "    vectors_pca = pca.fit_transform(vectors)\n",
    "\n",
    "    # TSNE final\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=42)\n",
    "    vectors_tsne = tsne.fit_transform(vectors_pca)\n",
    "\n",
    "    return vectors_tsne, labels\n",
    "\n",
    "# Limit the max number of words to avoid memory issues\n",
    "MAX_WORDS = 200\n",
    "\n",
    "# Reduce dimensions for Britney model\n",
    "vecs, labels = reduce_dimensions(britney_w2v_model, num_dimensions=3, max_words=MAX_WORDS)\n",
    "\n",
    "# Use Plotly to create a 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    x=vecs[:, 0],\n",
    "    y=vecs[:, 1],\n",
    "    z=vecs[:, 2],\n",
    "    text=labels\n",
    ")\n",
    "fig.update_traces(marker_size=3)\n",
    "\n",
    "# Open on browser\n",
    "fig.show(renderer=\"browser\")\n",
    "\n",
    "# Repeat for Taylor model\n",
    "vecs, labels = reduce_dimensions(taylor_w2v_model, num_dimensions=3, max_words=MAX_WORDS)\n",
    "fig = px.scatter_3d(\n",
    "    x=vecs[:, 0],\n",
    "    y=vecs[:, 1],\n",
    "    z=vecs[:, 2],\n",
    "    text=labels\n",
    ")\n",
    "fig.update_traces(marker_size=3)\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438fdd8",
   "metadata": {},
   "source": [
    "### 4. Conclusiones\n",
    "\n",
    "La primera conclusión que se puede sacar es que de un vistazo a la nube de palabras generada para cada artista se puede saber mucho de los temas que tratan sus canciones e incluso cómo tratan conceptos similares. Se puede intuir también el tipo de música o los sentimientos que esperan despertar cada una.\n",
    "\n",
    "También en función de la ubicación de las palabras se puede saber si son términos más genéricos, como temas presentes en varias de sus canciones o una temática central sobre la que hablan o se trata de términos más especializados, siendo que los primeros se encuentran en el centro de su respectiva nube y los segundos en la periferia.\n",
    "\n",
    "Por último, se puede acompañar lo visto con técnicas complementarias de clusterización como KMeans para poder entender cómo se agrupan naturalmente las palabras dentro de ejes temáticos o incluso poder llegar a segmentar las palabras en función de álbumes, por dar un ejemplo. Como se mencionó antes, es muy importante obtener la agrupación de las palabras en el espacio multidimensional original, donde la distancia entre palabras se preserva con mayor exactitud. Se presenta un ejemplo a continuación para completar esta explicación, eligiendo al azar 6 clústers para visualizar agrupaciones de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd09fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JuanI\\miniconda3\\envs\\NLP\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning:\n",
      "\n",
      "KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Train a KMeans model to find clusters in the word vectors\n",
    "N_CLUSTERS = 6\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(vecs)\n",
    "\n",
    "# Visualize\n",
    "fig = px.scatter_3d(\n",
    "    x=vecs[:, 0],\n",
    "    y=vecs[:, 1],\n",
    "    z=vecs[:, 2],\n",
    "    text=labels,\n",
    "    color=clusters.astype(str),    # color by cluster\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1\n",
    ")\n",
    "fig.update_traces(marker=dict(size=4, opacity=0.8))\n",
    "fig.update_layout(title=f\"KMeans clustering (k={N_CLUSTERS}) - Taylor Swift\")\n",
    "fig.show(renderer=\"browser\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
