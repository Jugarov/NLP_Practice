{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b482feff",
   "metadata": {},
   "source": [
    "## Consignas del desafío extra\n",
    "\n",
    "Construir QA Bot basado en el ejemplo del traductor pero con un dataset QA.\n",
    "\n",
    "## Recomendaciones:\n",
    "- MAX_VOCAB_SIZE = 8000\n",
    "- max_length ~ 10\n",
    "- Embeddings 300 Fasttext\n",
    "- n_units = 128\n",
    "- LSTM Dropout 0.2\n",
    "- Epochs 30~50\n",
    "\n",
    "## Preguntas interesantes:\n",
    "- Do you read?\n",
    "- Do you have any pet?\n",
    "- Where are you from?\n",
    "\n",
    "---\n",
    "\n",
    "## Desarrollo\n",
    "\n",
    "### 1. Obtención del Dataset\n",
    "\n",
    "Para entrenar un QA Bot basado en un modelo LSTM se necesita primero un dataset como el que provee la cátedra: [ConvAI2 (Conversational Intelligence Challenge 2)](http://convai.io/data/).\n",
    "Según la descripción provista por el propio dataset, este se compone de 4750 diálogos divididos en 4224 conversaciones entre una persona y un bot y 526 conversaciones entre dos personas. La estructura de los datos se compone de un texto que da contexto sobre la conversación que está teniendo lugar, una descripción de los agentes involucrados en la conversación, indicando si cada uno es un bot o una persona; y el desarrollo ordenado de la conversación junto con el usuario que dice cada frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a124cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context', 'users', 'evaluation', 'thread', 'dialogId'])\n",
      "Primeros 2 ejemplos (campos y tipos/longitudes):\n",
      "\n",
      "Ejemplo 0:\n",
      "  context: string de longitud 525\n",
      "  users: lista de longitud 2\n",
      "  evaluation: lista de longitud 2\n",
      "  thread: lista de longitud 1\n",
      "  dialogId: tipo <class 'int'>\n",
      "\n",
      "Ejemplo 1:\n",
      "  context: string de longitud 656\n",
      "  users: lista de longitud 2\n",
      "  evaluation: lista de longitud 2\n",
      "  thread: lista de longitud 1\n",
      "  dialogId: tipo <class 'int'>\n",
      "\n",
      "--- Estadísticas del dataset ---\n",
      "Número total de ejemplos: 2778\n",
      "Número de ejemplos con campos vacíos:\n",
      "\n",
      "Campos disponibles y cuántos ejemplos los contienen:\n",
      "  context: 2\n",
      "  users: 2\n",
      "  evaluation: 2\n",
      "  thread: 2\n",
      "  dialogId: 2\n"
     ]
    }
   ],
   "source": [
    "# Dataset_file\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "text_file = \"ConvAI2_dataset.json\"\n",
    "with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Observar los campos disponibles en cada linea del dataset\n",
    "print(data[0].keys())\n",
    "\n",
    "# Inicializar contadores\n",
    "field_lengths = Counter()\n",
    "num_empty = Counter()\n",
    "num_examples = len(data)\n",
    "\n",
    "# Revisar primeros 2 ejemplos\n",
    "print(\"Primeros 2 ejemplos (campos y tipos/longitudes):\")\n",
    "for i, sample in enumerate(data[:2]):\n",
    "    print(f\"\\nEjemplo {i}:\")\n",
    "    for k, v in sample.items():\n",
    "        if isinstance(v, list):\n",
    "            print(f\"  {k}: lista de longitud {len(v)}\")\n",
    "            if len(v) == 0:\n",
    "                num_empty[k] += 1\n",
    "        elif isinstance(v, str):\n",
    "            print(f\"  {k}: string de longitud {len(v)}\")\n",
    "            if len(v.strip()) == 0:\n",
    "                num_empty[k] += 1\n",
    "        else:\n",
    "            print(f\"  {k}: tipo {type(v)}\")\n",
    "        field_lengths[k] += 1\n",
    "\n",
    "# Estadísticas generales\n",
    "print(\"\\n--- Estadísticas del dataset ---\")\n",
    "print(f\"Número total de ejemplos: {num_examples}\")\n",
    "print(\"Número de ejemplos con campos vacíos:\")\n",
    "for k, count in num_empty.items():\n",
    "    print(f\"  {k}: {count} ({count/num_examples*100:.2f}%)\")\n",
    "\n",
    "# Contar los campos disponibles\n",
    "print(\"\\nCampos disponibles y cuántos ejemplos los contienen:\")\n",
    "for k, count in field_lengths.items():\n",
    "    print(f\"  {k}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb8fc1",
   "metadata": {},
   "source": [
    "### 2. Preprocesamiento\n",
    "\n",
    "Los datos crudos deben ser preprocesados y normalizados para facilitar la posterior tokenización del vocabulario. Para ello, se procesa el texto de muestra para:\n",
    "\n",
    "1. Escribir todo el texto en letra minúscula, normalizar los espacios entre palabras y eliminar caracteres especiales.\n",
    "2. Segmentar los turnos de conversación, para normalizar secuencias de dos frases seguidas donde hable una misma persona.\n",
    "3. Añadir tokens especiales\n",
    "4. Limitar la longitud de las estructuras encoder/decoder\n",
    "5. Filtrar ejemplos vacíos o muy largos\n",
    "\n",
    "Para entrenar luego el modelo encoder-decoder, se prepara el dataset de modo que la entrada de ejemplo del encoder sea la conversación en la que `users.userType` indique que sea humano y como target de salida del decoder la conversación en la que se indique que proviene de un bot. En caso de que la conversación se produzca entre dos humanos, se toma la conversación del primer usuario como entrada del encoder y la del segundo usuario como decoder, con el objetivo de no perder esa información y enseñar al bot a hablar con mayor naturalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3276532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import os\n",
    "\n",
    "# ------------------------\n",
    "# Configuración\n",
    "# ------------------------\n",
    "TEXT_FILE = \"ConvAI2_dataset.json\"\n",
    "OUTPUT_DIR = \"convai2_preprocessed\"\n",
    "MAX_LEN_ENCODER = 128\n",
    "MAX_LEN_DECODER = 64\n",
    "MIN_FREQ = 3\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    \"pad\": \"<pad>\",\n",
    "    \"unk\": \"<unk>\",\n",
    "    \"sos\": \"<sos>\",\n",
    "    \"eos\": \"<eos>\",\n",
    "    \"sep\": \"<sep>\",\n",
    "}\n",
    "\n",
    "# ------------------------\n",
    "# Normalización básica\n",
    "# ------------------------\n",
    "def normalize_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-zA-Z0-9áéíóúüñ!?.,' ]+\", \" \", s)  # mantener acentos y símbolos comunes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ------------------------\n",
    "# Construcción dataset\n",
    "# ------------------------\n",
    "def build_examples(data):\n",
    "    \"\"\"\n",
    "    Convierte dataset JSON en pares (encoder_input, decoder_target)\n",
    "    usando history y la respuesta target.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for sample in data:\n",
    "        thread = sample.get(\"thread\", [])\n",
    "        users = sample.get(\"users\", [])\n",
    "\n",
    "        if not thread or not users:\n",
    "            continue\n",
    "\n",
    "        # identificar bots y humanos\n",
    "        bot_ids = [u[\"id\"] for u in users if u.get(\"userType\") == \"Bot\"]\n",
    "        human_ids = [u[\"id\"] for u in users if u.get(\"userType\") == \"Human\"]\n",
    "\n",
    "        # si no hay bot, usar segundo humano como pseudo-bot\n",
    "        use_pseudo_bot = len(bot_ids) == 0 and len(human_ids) > 1\n",
    "        if use_pseudo_bot:\n",
    "            bot_ids = [human_ids[1]]  # segundo humano\n",
    "\n",
    "        # crear ejemplos por turno\n",
    "        history = []\n",
    "        for turn in thread:\n",
    "            text = turn.get(\"text\", \"\").strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            uid = turn.get(\"userId\")\n",
    "\n",
    "            # normalizar\n",
    "            norm_text = normalize_text(text)\n",
    "\n",
    "            # si es turno de bot -> decoder_target\n",
    "            if uid in bot_ids and history:\n",
    "                encoder_text = f\" {SPECIAL_TOKENS['sep']} \".join(history)\n",
    "                examples.append({\n",
    "                    \"encoder_text\": encoder_text,\n",
    "                    \"decoder_text\": norm_text\n",
    "                })\n",
    "\n",
    "            # agregar turno al historial\n",
    "            history.append(norm_text)\n",
    "            # limitar longitud histórica para encoder\n",
    "            if len(history) > 10:  # por ejemplo, últimos 10 turnos\n",
    "                history = history[-10:]\n",
    "\n",
    "    return examples\n",
    "\n",
    "# Construir pares encoder-decoder\n",
    "examples = build_examples(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f6f72",
   "metadata": {},
   "source": [
    "### 3. Tokenización\n",
    "\n",
    "Tras limpiar el contenido de las conversaciones, normalizar el texto y separar las entradas y salidas del encoder y del decoder, se continúa con la tokenización del texto y la generación de diccionarios para trabajar con secuencias más aptas para el uso de modelos. Para un manejo del lenguaje más robusto y naturalizado, así como para evitar `<unk>`, se tokeniza el corpus utilizando la librería *spaCy* como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff356021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocesamiento completo\n",
      "{'num_examples': 13889, 'vocab_size': 8505, 'max_len_encoder': 128, 'max_len_decoder': 64}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# ------------------------\n",
    "# Cargar modelo spaCy\n",
    "# ------------------------\n",
    "# Descargar modelo si no lo tenés:\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ------------------------\n",
    "# Tokenización con spaCy\n",
    "# ------------------------\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza un texto usando spaCy.\n",
    "    Devuelve una lista de tokens limpios.\n",
    "    Compatible con embeddings FastText.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc if not token.is_space]\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(examples):\n",
    "    counter = collections.Counter()\n",
    "    for ex in examples:\n",
    "        counter.update(tokenize(ex[\"encoder_text\"]))\n",
    "        counter.update(tokenize(ex[\"decoder_text\"]))\n",
    "\n",
    "    # aplicar min_freq\n",
    "    tokens = [tok for tok, c in counter.items() if c >= MIN_FREQ]\n",
    "    tokens = tokens[:VOCAB_SIZE - len(SPECIAL_TOKENS)]\n",
    "\n",
    "    vocab = list(SPECIAL_TOKENS.values()) + tokens\n",
    "    token2id = {tok: i for i, tok in enumerate(vocab)}\n",
    "    id2token = {i: tok for tok, i in token2id.items()}\n",
    "    return token2id, id2token\n",
    "\n",
    "def numericalize(tokens, token2id, max_len, is_decoder=False):\n",
    "    ids = []\n",
    "    if is_decoder:\n",
    "        ids.append(token2id[SPECIAL_TOKENS[\"sos\"]])\n",
    "    for t in tokens:\n",
    "        ids.append(token2id.get(t, token2id[SPECIAL_TOKENS[\"unk\"]]))\n",
    "    if is_decoder:\n",
    "        ids.append(token2id[SPECIAL_TOKENS[\"eos\"]])\n",
    "\n",
    "    length = len(ids)\n",
    "    ids = ids[:max_len]\n",
    "\n",
    "    while len(ids) < max_len:\n",
    "        ids.append(token2id[SPECIAL_TOKENS[\"pad\"]])\n",
    "\n",
    "    return ids, min(length, max_len)\n",
    "\n",
    "# Vocabulario\n",
    "token2id, id2token = build_vocab(examples)\n",
    "\n",
    "processed = []\n",
    "for ex in examples:\n",
    "    enc_tokens = tokenize(ex[\"encoder_text\"])\n",
    "    dec_tokens = tokenize(ex[\"decoder_text\"])\n",
    "\n",
    "    enc_ids, enc_len = numericalize(enc_tokens, token2id, MAX_LEN_ENCODER)\n",
    "    dec_ids, dec_len = numericalize(dec_tokens, token2id, MAX_LEN_DECODER, is_decoder=True)\n",
    "\n",
    "    processed.append({\n",
    "        \"encoder_ids\": enc_ids,\n",
    "        \"encoder_len\": enc_len,\n",
    "        \"decoder_ids\": dec_ids,\n",
    "        \"decoder_len\": dec_len\n",
    "    })\n",
    "\n",
    "# Guardar\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(f\"{OUTPUT_DIR}/token2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(token2id, f)\n",
    "with open(f\"{OUTPUT_DIR}/id2token.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2token, f)\n",
    "with open(f\"{OUTPUT_DIR}/dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(processed, f)\n",
    "\n",
    "stats = {\n",
    "    \"num_examples\": len(processed),\n",
    "    \"vocab_size\": len(token2id),\n",
    "    \"max_len_encoder\": MAX_LEN_ENCODER,\n",
    "    \"max_len_decoder\": MAX_LEN_DECODER,\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/stats.json\", \"w\") as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text.lower() for tok in nlp(text) if not tok.is_space]\n",
    "\n",
    "print(\"✅ Preprocesamiento completo\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b5f5c",
   "metadata": {},
   "source": [
    "### 4. Embeddings\n",
    "\n",
    "Con los diccionarios de vocabulario ya creados, se codifican embeddings usando estos tokens como base y con ayuda del módulo `gensim.Fastext` de *Gensim*, que se elige por su robustez y capacidad de adaptación frente a palabras por fuera del vocabulario de muestra, lo cual es probable que ocurra en un bot conversacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19429802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Extraer todas las frases tokenizadas del dataset\n",
    "all_sentences = []\n",
    "\n",
    "for ex in processed:  # processed = lista de diccionarios con 'encoder_ids' y 'decoder_ids'\n",
    "    enc_tokens = [id2token[i] for i in ex['encoder_ids'] if i != token2id['<pad>']]\n",
    "    dec_tokens = [id2token[i] for i in ex['decoder_ids'] if i != token2id['<pad>']]\n",
    "    \n",
    "    all_sentences.append(enc_tokens)\n",
    "    all_sentences.append(dec_tokens)\n",
    "\n",
    "\n",
    "# Entrenar FastText\n",
    "ft_model = FastText(\n",
    "    sentences=all_sentences,\n",
    "    vector_size=100,   # dimensión de los embeddings\n",
    "    window=5,          # contexto\n",
    "    min_count=1,       # ya tenemos vocab, poner 1 para incluir todos los tokens\n",
    "    sg=1,              # 1 = skip-gram, 0 = CBOW\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Guardar modelo\n",
    "ft_model.save(\"fasttext_convai2.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5c0ab",
   "metadata": {},
   "source": [
    "Con el embedding ya creado, se traducen todos los tensores tokenizados a tensores con los embeddings que ha encontrado FastText, que serán los tensores con los que se alimenta el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde5247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8505, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = len(token2id)\n",
    "embedding_dim = ft_model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for token, idx in token2id.items():\n",
    "    if token in ft_model.wv:\n",
    "        embedding_matrix[idx] = ft_model.wv[token]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))  # fallback\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "print(embedding_matrix.shape)  # (vocab_size, embedding_dim)\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01d7ee",
   "metadata": {},
   "source": [
    "### 5. Dataloaders\n",
    "\n",
    "Para entrenar el modelo encoder-decoder se facilita la ingesta de datos a través del armado de un Dataloader. Este dataloader debe devolver tensores `encoder_input` y `decoder_input` para el consumo de los datos y `decoder_target` como respuesta de validación con la que ajustar el modelo. Para ello, es necesario eliminar los tokens `<eos>` y `<sos>` en la entrada y salida del decoder, respectivamente; de modo que las listas de entrenamiento sean del mismo tamaño pero con un desfasaje entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21cf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BotDataset(Dataset):\n",
    "    def __init__(self, data, pad_idx):\n",
    "        self.data = data\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        enc = ex[\"encoder_ids\"]\n",
    "        dec = ex[\"decoder_ids\"]\n",
    "\n",
    "        # Entrada al decoder: sin el último token (<eos>)\n",
    "        dec_input = dec[:-1]\n",
    "        # Target del decoder: sin el primero (<sos>)\n",
    "        dec_target = dec[1:]\n",
    "\n",
    "        return torch.tensor(enc, dtype=torch.long), torch.tensor(dec_input, dtype=torch.long), torch.tensor(dec_target, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a3604",
   "metadata": {},
   "source": [
    "Como las secuencias tienen longitudes variables, se requiere construir un padding dinámico que llene los espacios vacíos cuando sea necesario. Esto se traduce en una *collate function* personalizada que permita hacer esto en la construcción de secuencias del Dataloader. La función de padding es un asset más que se utiliza directamente desde PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6bba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "PAD_IDX = token2id[PAD_TOKEN]\n",
    "SOS_IDX = token2id[SOS_TOKEN]\n",
    "EOS_IDX = token2id[EOS_TOKEN]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    encoders, dec_inputs, dec_targets = zip(*batch)\n",
    "    \n",
    "    enc_padded = pad_sequence(encoders, batch_first=True, padding_value=PAD_IDX)\n",
    "    dec_in_padded = pad_sequence(dec_inputs, batch_first=True, padding_value=PAD_IDX)\n",
    "    dec_tgt_padded = pad_sequence(dec_targets, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "    return enc_padded, dec_in_padded, dec_tgt_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169eab8",
   "metadata": {},
   "source": [
    "Habiendo creado estas estructuras, se genera el dataloader que tome como base los embeddings preparados, separando previamente el mismo en conjuntos de entrenamiento y validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "698ba927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "dataset = BotDataset(processed, PAD_IDX)\n",
    "\n",
    "# train-val split: 80/20 split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2c3bd",
   "metadata": {},
   "source": [
    "### 6. Estructura del modelo\n",
    "\n",
    "Con todos los datos de alimentación y comparación preparados, solo resta crear la estructura del modelo y entrenarlo. El modelo *seq2seq* consiste en un modelo many-to-many que traduce una secuencia de un tensor a otro, pero procesando todo el tensor de entrada a la vez y considerando la relación y orden de los elementos del tensor y no solo la aparición o no de los mismos. El mecanismo que produce esta transformación se basa en una arquitectura encoder-decoder, donde el segundo trata de recrear el mapeo que realiza el primero sobre la información de entrada para entender mejor el contexto de escritura en el que se trabaja para poder brindar una salida de mejor calidad.\n",
    "\n",
    "Como se mencionó antes, el orden de los elementos del tensor es relevante para poder obtener una mejor salida, por lo que la utilización de capas LSTM resulta de gran ayuda en la obtención de una salida de calidad.\n",
    "\n",
    "Se destaca que en lugar de entrenar la capa de embeddings desde cero como en un modelo normal, se importa la capa de embeddings ya generada con FastText para la codificación de los elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a8eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding.embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fccff",
   "metadata": {},
   "source": [
    "Para el decodificador se incorpora además un mecanismo de atención que permita al modelo tener más presente el contexto general de la conversación y no solo depender de la última información recibida. Vale la pena mencionar que se escoge una técnica de atención conocido como **[Atención Bahdanau](https://medium.com/@abhishekjainindore24/everything-about-attention-mechanism-bahdanau-attention-and-luong-attention-f76e63b702ca)**, que permite también destacar los tokens que más información aportan al contexto de la conversación que se está teniendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063a2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: [batch, dec_hidden_dim]\n",
    "        # encoder_outputs: [batch, src_len, enc_hidden_dim]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # Repetir hidden para cada paso de la secuencia\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch, src_len, dec_hidden_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch, src_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding, enc_hidden_dim, dec_hidden_dim, output_dim, attention, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.attention = attention\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding.embedding_dim + enc_hidden_dim,\n",
    "            dec_hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim + dec_hidden_dim + embedding.embedding_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs, mask=None):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]\n",
    "\n",
    "        attn_weights = self.attention(hidden[-1], encoder_outputs, mask).unsqueeze(1)  # [batch, 1, src_len]\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [batch, 1, enc_hidden_dim]\n",
    "\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "\n",
    "        # Salida final\n",
    "        output_pred = self.fc_out(torch.cat((output, context, embedded), dim=2).squeeze(1))\n",
    "        return output_pred, hidden, cell, attn_weights.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3bfa2",
   "metadata": {},
   "source": [
    "La estructura del modelo completo seq2seq queda definido entonces como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8244f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def create_mask(self, src):\n",
    "        return (src != self.pad_idx).to(self.device)\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        mask = self.create_mask(src)\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs, mask)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eab68d",
   "metadata": {},
   "source": [
    "### 7. Entrenamiento del modelo\n",
    "\n",
    "Con la estructura y los datos ya preparados, solo queda definir una función de entrenamiento para configurar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db66f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, clip=1.0):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Entrenando\", leave=False):\n",
    "        src, trg_in, trg_out = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg_in)  # [batch, trg_len, vocab_size]\n",
    "\n",
    "        # Ignorar el primer token (<sos>)\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), trg_out.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping para estabilidad\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Validando\", leave=False):\n",
    "        src, trg_in, trg_out = [x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(src, trg_in)\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), trg_out.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, n_epochs=10, clip=1.0):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, clip)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # ---- Curvas de pérdida ----\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.title(\"Curvas de pérdida\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e35664",
   "metadata": {},
   "source": [
    "Por último, se inicializan todas las estructuras del modelo y se usa la función de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858151df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-3\u001b[39m)\n\u001b[32m     17\u001b[39m criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, device, n_epochs, clip)\u001b[39m\n\u001b[32m     53\u001b[39m val_losses = []\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, n_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     val_loss = evaluate(model, val_loader, criterion, device)\n\u001b[32m     59\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion, device, clip)\u001b[39m\n\u001b[32m      8\u001b[39m epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc=\u001b[33m\"\u001b[39m\u001b[33mEntrenando\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     src, trg = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mencoder_input\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.to(device), batch[\u001b[33m'\u001b[39m\u001b[33mdecoder_target\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     13\u001b[39m     optimizer.zero_grad()\n\u001b[32m     14\u001b[39m     output = model(src, trg)  \u001b[38;5;66;03m# [batch, trg_len, vocab_size]\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(token2id)\n",
    "OUTPUT_DIM = len(token2id)\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.3\n",
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", DEVICE)\n",
    "\n",
    "attn = Attention(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "encoder = Encoder(embedding_layer, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
    "decoder = Decoder(embedding_layer, HIDDEN_SIZE, HIDDEN_SIZE, OUTPUT_DIM, attn, NUM_LAYERS, DROPOUT)\n",
    "\n",
    "model = Seq2SeqModel(encoder, decoder, PAD_IDX, DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    DEVICE,\n",
    "    n_epochs=EPOCHS,\n",
    "    clip=1.0\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), \"conv_bot_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071520ab",
   "metadata": {},
   "source": [
    "### 8. Inferencia\n",
    "\n",
    "Con el modelo ya entrenado, se puede utilizar el mismo para inferir nuevas respuestas. Para generalizar un poco más las respuestas se usa un algoritmo *beam search* para entregar la frase de salida en lugar de una búsqueda *greedy* que puede llevar a \"alucinaciones\" en la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def beam_search(model, src_tensor, src_len, vocab, beam_width=3, max_len=20, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_tensor = src_tensor.unsqueeze(0).to(device)\n",
    "        src_len = torch.tensor([src_len]).to(device)\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "        \n",
    "        # Inicializamos el haz con el token <SOS>\n",
    "        beams = [(torch.tensor([[vocab[\"<sos>\"]]]).to(device), hidden, 0)]  # (seq, hidden, score)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, hidden, score in beams:\n",
    "                last_token = seq[-1, -1].unsqueeze(0)\n",
    "\n",
    "                if last_token.item() == vocab[\"<eos>\"]:\n",
    "                    new_beams.append((seq, hidden, score))\n",
    "                    continue\n",
    "\n",
    "                output, hidden, cell, attn_weights = model.decoder(\n",
    "                    last_token, hidden, cell, encoder_outputs\n",
    "                )\n",
    "                log_probs = F.log_softmax(output, dim=1)\n",
    "\n",
    "                # Penalizar tokens desconocido en la respuesta\n",
    "                log_probs[:, vocab[\"<unk>\"]] -= 10.0\n",
    "\n",
    "                # Expandimos los beam candidates\n",
    "                topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
    "\n",
    "                for k in range(beam_width):\n",
    "                    next_token = topk_indices[0][k].unsqueeze(0).unsqueeze(0)\n",
    "                    next_seq = torch.cat([seq, next_token], dim=1)\n",
    "                    next_score = score + topk_log_probs[0][k].item()\n",
    "                    new_beams.append((next_seq, hidden, next_score))\n",
    "\n",
    "            # Ordenamos y mantenemos los mejores beam_width candidatos\n",
    "            beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:beam_width]\n",
    "\n",
    "        # Retorna la mejor secuencia (la de mayor score)\n",
    "        best_seq = beams[0][0].squeeze().tolist()\n",
    "        return best_seq\n",
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "tokens = [token2id.get(tok, token2id[\"<unk>\"]) for tok in input_text.split()]\n",
    "indices = [token2id.get(t, token2id[\"<unk>\"]) for t in tokens]\n",
    "src_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "predicted_ids = beam_search(model, src_tensor, len(indices), token2id, beam_width=10)\n",
    "predicted_tokens = [id2token[i] for i in predicted_ids if i not in [token2id[\"<sos>\"], token2id[\"<eos>\"], token2id[\"<pad>\"]]]\n",
    "\n",
    "print(\"Respuesta generada:\", \" \".join(predicted_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLO_Old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
